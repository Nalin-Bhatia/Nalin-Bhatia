{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":78156,"sourceType":"datasetVersion","datasetId":44109},{"sourceId":8585217,"sourceType":"datasetVersion","datasetId":5134769}],"dockerImageVersionId":30715,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import *\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimagesPath = '/kaggle/input/utkface-new/UTKFace/'\nallImages = os.listdir(imagesPath)\ntotalNbrOfImages = len(allImages)\nprint(\"Total count of images : \", totalNbrOfImages)","metadata":{"_uuid":"6ae3c98a-3690-4e42-aab6-f850449a7666","_cell_guid":"27416d81-1c2f-4761-98e0-944791226b75","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-03T13:36:46.176539Z","iopub.execute_input":"2024-06-03T13:36:46.177292Z","iopub.status.idle":"2024-06-03T13:36:46.198692Z","shell.execute_reply.started":"2024-06-03T13:36:46.177257Z","shell.execute_reply":"2024-06-03T13:36:46.197596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating the Image, Ages and the Genders Array:\nupdatedImages = []\nallAgeNbrs = []\nallGenders = []\ncounter = 0\n\nfor file in allImages:\n    #Use this if condition to restrict input data to 30 images\n    #if (counter >= 30):\n    #    break\n\n    image = cv2.imread(imagesPath+file,0)\n    image = cv2.resize(image,dsize=(200,200))\n    image = image.reshape((image.shape[0],image.shape[1],1))\n    updatedImages.append(image)\n    split_var = file.split('_')\n    allAgeNbrs.append(split_var[0])\n    allGenders.append(int(split_var[1]))\n    \n    #counter += 1\n    #print(\"allAgeNbrs : \", allAgeNbrs)\n    #print(\"allGenders : \", allGenders)","metadata":{"_uuid":"22731355-0891-4661-b187-f6ed760257af","_cell_guid":"8780ef27-7fed-4781-b8e4-81e9b1ae11c8","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-03T13:36:46.200450Z","iopub.execute_input":"2024-06-03T13:36:46.200799Z","iopub.status.idle":"2024-06-03T13:39:07.886029Z","shell.execute_reply.started":"2024-06-03T13:36:46.200762Z","shell.execute_reply":"2024-06-03T13:39:07.885121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining the function to display Images:\n\ndef displayImage(img):\n    plt.imshow(img[:,:,0])\n    plt.set_cmap('gray')\n    plt.show()\n    \n# Function for defining the Age Groups:\n\ndef age_group(age):\n    if age >=0 and age < 18:\n        return 1\n    elif age < 30:\n        return 2\n    elif age < 80:\n        return 3\n    else:\n        return 4","metadata":{"_uuid":"69f3d950-596e-417d-be31-d70346174fbb","_cell_guid":"10e33346-9a94-4824-a5e1-e2123d792149","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-03T13:39:07.887343Z","iopub.execute_input":"2024-06-03T13:39:07.888002Z","iopub.status.idle":"2024-06-03T13:39:07.894166Z","shell.execute_reply.started":"2024-06-03T13:39:07.887965Z","shell.execute_reply":"2024-06-03T13:39:07.893195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Displaying an Image from the Dataset:\nimageCtr = 5\nsampleImage = updatedImages[imageCtr]\nprint(\"Gender:\",allGenders[imageCtr],\"Age:\",allAgeNbrs[imageCtr])\ndisplayImage(sampleImage)","metadata":{"_uuid":"e05a18dd-8ad5-42ca-bc6c-f2d6126e34eb","_cell_guid":"c895ef6c-4e33-467a-9ff3-8119d49b4240","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-03T13:39:07.896664Z","iopub.execute_input":"2024-06-03T13:39:07.897011Z","iopub.status.idle":"2024-06-03T13:39:08.099155Z","shell.execute_reply.started":"2024-06-03T13:39:07.896979Z","shell.execute_reply":"2024-06-03T13:39:08.098266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pre-processing:\n\nlabels = np.zeros((totalNbrOfImages,2),dtype='float32')\nfeatures = np.zeros((totalNbrOfImages,sampleImage.shape[0],sampleImage.shape[1],1),dtype = 'float32')\n\nfor i in range(totalNbrOfImages):\n    labels[i,0] = age_group(int(allAgeNbrs[i])) / 4\n    labels[i,1] = int(allGenders[i])\n    features[i] = updatedImages[i]\n\nfeatures = features / 255\n\ndisplayImage(features[15])","metadata":{"_uuid":"005138b4-3f07-46ad-86b5-a13946209d8f","_cell_guid":"408c340a-d6a4-48fc-a3e6-373ffedf8301","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-03T13:39:08.100131Z","iopub.execute_input":"2024-06-03T13:39:08.100402Z","iopub.status.idle":"2024-06-03T13:39:08.706283Z","shell.execute_reply.started":"2024-06-03T13:39:08.100378Z","shell.execute_reply":"2024-06-03T13:39:08.705269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Testing markdown..........","metadata":{"_uuid":"58b5c770-418a-406f-a88b-02cb5ae2ede2","_cell_guid":"f8a74a50-5c37-4705-b9ef-9f25272b3a3b","trusted":true}},{"cell_type":"code","source":"# Splitting the Original Dataset into Training and Testing Dataset:\n\nx_train, x_test, y_train, y_test = train_test_split(features, labels, test_size=0.2,shuffle  = True)\nprint(\"Samples in Training:\",x_train.shape[0])\nprint(\"Samples in Testing:\",x_test.shape[0])","metadata":{"_uuid":"73814d9d-8bb3-4b5a-8260-31eb1de1ae2d","_cell_guid":"8b75a9b6-5057-498d-962a-a4aa8f11b565","collapsed":false,"execution":{"iopub.status.busy":"2024-06-03T13:39:08.707623Z","iopub.execute_input":"2024-06-03T13:39:08.708041Z","iopub.status.idle":"2024-06-03T13:39:08.833648Z","shell.execute_reply.started":"2024-06-03T13:39:08.708006Z","shell.execute_reply":"2024-06-03T13:39:08.832696Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining the Neural Network Layers:\n\ninputLayer = Input(shape=(200,200,1))\nconvLayer1 = Conv2D(32, kernel_size=(3, 3),activation='relu')(inputLayer)\nconvLayer2 = Conv2D(64, kernel_size=(3, 3),activation='relu')(convLayer1)\nmaxPoolLayer1 = MaxPooling2D(pool_size=(2, 2))(convLayer2)\nconvLayer3 = Conv2D(128, kernel_size=(3, 3),activation='relu')(maxPoolLayer1)\nmaxPoolLayer2 = MaxPooling2D(pool_size=(2, 2))(convLayer3)\ndropoutLayer = Dropout(0.25)(maxPoolLayer2)\nflattenlayer = Flatten()(dropoutLayer)\n\n#defining layers for age modeling \ndropout = Dropout(0.5)\nage_model = Dense(128, activation='relu')(flattenlayer)\nage_model = dropout(age_model)\nage_model = Dense(64, activation='relu')(age_model)\nage_model = dropout(age_model)\nage_model = Dense(32, activation='relu')(age_model)\nage_model = dropout(age_model)\nage_model = Dense(1, activation='relu')(age_model)\n\n#defining layers for gender modeling \ndropout = Dropout(0.5)\ngender_model = Dense(128, activation='relu')(flattenlayer)\ngender_model = dropout(gender_model)\ngender_model = Dense(64, activation='relu')(gender_model)\ngender_model = dropout(gender_model)\ngender_model = Dense(32, activation='relu')(gender_model)\ngender_model = dropout(gender_model)\ngender_model = Dense(16, activation='relu')(gender_model)\ngender_model = dropout(gender_model)\ngender_model = Dense(8, activation='relu')(gender_model)\ngender_model = dropout(gender_model)\ngender_model = Dense(1, activation='sigmoid')(gender_model)","metadata":{"_uuid":"3825918d-178e-4422-bff6-c0b0910f7c7c","_cell_guid":"1f516d5c-81f4-4906-85d5-06bcf71b0952","collapsed":false,"execution":{"iopub.status.busy":"2024-06-03T13:39:08.834877Z","iopub.execute_input":"2024-06-03T13:39:08.835248Z","iopub.status.idle":"2024-06-03T13:39:09.662788Z","shell.execute_reply.started":"2024-06-03T13:39:08.835213Z","shell.execute_reply":"2024-06-03T13:39:09.662031Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compile the Model:\n\nCNNModel = Model(inputs=inputLayer, outputs=[age_model,gender_model])\nCNNModel.compile(\n    optimizer = 'adam', \n    loss =['mse','binary_crossentropy'],\n    metrics=['mean_squared_error', 'binary_accuracy']\n)\n\nCNNModel.summary()","metadata":{"_uuid":"25a9e8fa-da4e-4ab1-8605-58a37f202884","_cell_guid":"735e4ccb-98f2-4985-ac25-3cbd7dd76a63","collapsed":false,"execution":{"iopub.status.busy":"2024-06-03T13:39:09.663954Z","iopub.execute_input":"2024-06-03T13:39:09.664289Z","iopub.status.idle":"2024-06-03T13:39:09.719662Z","shell.execute_reply.started":"2024-06-03T13:39:09.664263Z","shell.execute_reply":"2024-06-03T13:39:09.718802Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the Model:\n\nh = CNNModel.fit(\n    x_train,\n    [y_train[:, 0], y_train[:, 1]],  \n    validation_data=(x_test, [y_test[:, 0], y_test[:, 1]]),\n    epochs=40,\n    batch_size=128,\n    shuffle=True \n)","metadata":{"_uuid":"6ff25834-3c80-4f16-b469-d70af6b3195c","_cell_guid":"575eb436-4d19-4fa5-bbff-70dfd9b65a16","collapsed":false,"execution":{"iopub.status.busy":"2024-06-03T13:39:09.720729Z","iopub.execute_input":"2024-06-03T13:39:09.721025Z","iopub.status.idle":"2024-06-03T13:44:04.369166Z","shell.execute_reply.started":"2024-06-03T13:39:09.721001Z","shell.execute_reply":"2024-06-03T13:44:04.368338Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CNNModel.save('nalin_model_pretrain.h5')","metadata":{"_uuid":"f93239ee-8c13-47fe-9f41-0adb73c95503","_cell_guid":"13f5af0e-6764-4649-a141-b57c95ff2286","collapsed":false,"execution":{"iopub.status.busy":"2024-06-03T13:44:04.372274Z","iopub.execute_input":"2024-06-03T13:44:04.372542Z","iopub.status.idle":"2024-06-03T13:44:04.568630Z","shell.execute_reply.started":"2024-06-03T13:44:04.372519Z","shell.execute_reply":"2024-06-03T13:44:04.567769Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining the functions for getting the Predictions:\n\ndef get_age(distr):\n    distr = distr*4\n    if distr >= 0.65 and distr <= 1.4:return \"0-18\"\n    if distr >= 1.65 and distr <= 2.4:return \"19-30\"\n    if distr >= 2.65 and distr <= 3.4:return \"31-80\"\n    if distr >= 3.65 and distr <= 4.4:return \"80 +\"\n    return \"Unknown\"\n\ndef get_gender(prob):\n    if prob < 0.5:return \"Male\"\n    else: return \"Female\"\n\ndef get_result(sample):\n    sample = sample/255\n    val = CNNModel.predict( np.array([ sample ]) )\n    age = get_age(val[0])\n    gender = get_gender(val[1])\n    print(\"Values:\",val,\"\\nPredicted Gender:\",gender,\"Predicted Age:\",age)","metadata":{"_uuid":"f1f3bb9c-cb95-4d35-b482-5fc6624597bd","_cell_guid":"f9231704-33d4-4753-9bef-3391963b074b","collapsed":false,"execution":{"iopub.status.busy":"2024-06-03T13:44:04.569795Z","iopub.execute_input":"2024-06-03T13:44:04.570106Z","iopub.status.idle":"2024-06-03T13:44:04.577259Z","shell.execute_reply.started":"2024-06-03T13:44:04.570079Z","shell.execute_reply":"2024-06-03T13:44:04.576390Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Taking the predictions for a set of sample data points:\n\nindexes = [2520,5140,8005,12010]\nfor idx in indexes:\n    sample = updatedImages[idx]\n    displayImage(sample)\n    print(\"Actual Gender:\",get_gender(allGenders[idx]),\"Age:\",allAgeNbrs[idx])\n    res = get_result(sample)\n    \n    \nmyImagespath = '/kaggle/input/my-photos-1/'\nmyImages = os.listdir(myImagespath)\nfor myImage in myImages:\n\n    image = cv2.imread(myImagespath+myImage,0)\n    image = cv2.resize(image,dsize=(200,200))\n    image = image.reshape((image.shape[0],image.shape[1],1))\n    #updatedMyImages.append(image)\n    displayImage(image)\n    res = get_result(image)","metadata":{"_uuid":"bc7a6d27-ace5-4920-809d-ffac62b07664","_cell_guid":"6e6f3585-536a-4495-960f-a4561d30e28c","collapsed":false,"execution":{"iopub.status.busy":"2024-06-03T13:44:04.578537Z","iopub.execute_input":"2024-06-03T13:44:04.578908Z","iopub.status.idle":"2024-06-03T13:44:07.535839Z","shell.execute_reply.started":"2024-06-03T13:44:04.578874Z","shell.execute_reply":"2024-06-03T13:44:07.534976Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"_uuid":"7f322a69-15ae-4ac7-9971-57e264b2f3ff","_cell_guid":"77eba583-aa0d-4974-bf27-65f216dcd741","trusted":true}}]}